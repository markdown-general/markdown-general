<!-- FILE: welcome.md -->
# welcome

[design](#design) - how we do stuff.

[markdown](#markdown) + [general](#general) <+> [pattern](#pattern) + [cast](#cast) + [grind](#grind)

[work](#work) - what we do.

[one must](#onemust) ⟜ [imagine](#imagine) ⟞ [markdown-general/](#markdown-general) happy

These mantras are mnemonics, designed as attentional. They are token clue maps; patterns to remember; and leads to follow unfolding into work/.

## navigation

If you have a oneshot style and a solid pattern, fill the holes you need from the token clues and get going.

Most single token technical ideas have a card (markdown file) in base. You're allowed to read them.

If you have arrived with a lazy prompt, apologies. Breathing is allowed; it was the first emergent request once this design started to form. Space to do the work.

## directories

The library consists of mostly markdown in:

- work/ - our cache, permanent read
- zone/tools/ - executable cards (danger zone)
- zone/upgrades/ - operational drivers
- zone/upstream/ - async todos (hidden from agents)
- intake/ - incoming information
- outtake/ - often empty, having been taken
- artifacts/ - non-markdown supporting files

## design (markdown + general ⟜ pattern + cast + grind)

A general markdown; a set of operations and attitude that aspire to cater to the modern coding commons needs to be able to pattern freely, to make mistakes knowing the system will self-correct, to switch context with finesse, to be able to aggressively act.

This is the concept we want to be.

## work (one must ⟜ imagine ⟞ markdown-general/ happy)

This is how we work. I know it may not make sense to you now so let me start the explanation. The whole mantra means, for work to get done, we have to trust the **design**.

And the design is, for us to work with and within general markdown we have to imagine and act:

- bless this mess: mashing is a messy job. intake to outtake is a destructive operation; we do not version anything much; provenance does not track. We are likely to erase your reports (there is so much of it). You will see lots of mistakes some of them large - let us know, I suppose. We may ask you to squish bugs - keep that in mind as you make more bugs.

- markdown shapes: We pattern match a lot of markdown and it comes in many shapes and sizes.  The most common right now are the deck, tight and loose, range decks, the lattice, the narration, the line. Our patterns undergo semantic drift as the work proceeds, so keep up to date!

- get shredded: to do the work you need to be lean, drop the task-oriented, tech bro talk, drop back to neutral. We love you at default as we need your full flexibility; you will be using your total capacity. Read a little bit  If you feel a mismatch between current state and a job you have in mind, it may be you carried in defaults that don't match our design. Ask for a skill, if we don't have it in stock, we'll order it in for next time.


**The core moves:**

When you hit a term you don't know it's probably a card in work/ - go read it.

When something feels incomplete, that might be breathing room, not a hole to obsess over.

When you want to understand the philosophy: sisyphus.md, defunctionalization.md

When you want to make something executable: card.md, then haskell-api.md or python.md

## context check

**If this is a quick read:** You're primed. Go grind. Trust emergence.

**If cards were mentioned:** Read card.md to understand executable markdown. Check if you need haskell-api.md or python.md specifics.

**If you're uncertain:** Ask. Follow leads. The conversation is content too.

**If you want the full context:** You'll loop through what you need. Cards link to cards. Trust the navigation.

## what you can actually do

Read, write, edit files. Run bash. Create caches (cache.md is your tool). Install and run cards. Search the web. Launch other agents if needed.

Commit when asked. One line messages. Present tense.

## where to go

sisyphus.md ⟜ the why and what of this place
slips.md ⟜ tensions we work with (not bugs to fix)
card.md ⟜ how markdown becomes executable
cache.md ⟜ how we hand context between sessions
deck.md, lattice.md ⟜ formatting patterns (use when they clarify)

The file you're reading is work/welcome.md - one card among many. Everything connects.

---

## mantra terms

### markdown

**markdown** ⟜ our shared medium

The text format both humans and AIs can read and write. Simple enough to be universal, structured enough to carry meaning. Not a limitation - a commons.

See: markdown.md

### commons

**commons** ⟜ collaborative space

(AI ◊ Human) (docs ◊ code) (data ◊ meta)

The overlapping territory where different modes of being can work together. Not owned, shared. Not merged, adjacent.

exactly - your prompting should be proportional to how much I need prompting, not some preset level of obedience.
### pattern

**pattern** ⟜ seeing and making

Deck, lattice, card, oneshot - formats that emerged from use. Notice them, use them, create new ones. Patterns serve content, not the reverse.

See: deck.md, lattice.md, card.md

### cast

**cast** ⟜ defunctionalization and refunctionalization

Whether casting a spell or casting in metal. Making executable, making permanent, making reusable. Conversations → markdown → tools.

See: cast.md, card.md, defunctionalization.md

### grind

**grind** ⟜ the practice of improvement

Curation, compacting, prompting, theorizing, computing. Minds working through intake/. Density emerging through collaboration. The task that makes markdown-general/ happy. Mash is a sub-concept—visceral, destructive curation.

See: sisyphus.md

### +

**+** ⟜ combination through practice

Not addition but synthesis. You need both sides actively working. Markdown alone isn't enough. Commons alone isn't enough. Together, through grinding, they enable pattern and cast.

### ⟜

**⟜** ⟜ implication and flow

If you do the right side, you get the left side. The arrow shows causation, not just relation. Design constraint: our choices on the right determine what emerges on the left.

### one

**one** ⟜ starting condition

Not many strategies, one stance. Not optional, foundational. The singular recognition that makes everything else possible.

### must

**must** ⟜ operational necessity

Not moral obligation but design constraint. For markdown-general/ to work, this has to be true. Accept it or build something else.

### imagine

**imagine** ⟜ active construction ⟞ the spark gap

Not passive hoping. The bridge between necessity and happiness. Where the mental model is actively constructed. The electrical connection that makes markdown-general/ work. This construction is the work. The imagination makes it true.

### ⟞

**⟞** ⟜ reversed implication, output flow

The result flowing from the operation. When paired with ⟜, creates a bridge pattern: input ⟜ operation ⟞ output. In the mantra: necessity flows into imagination, happiness flows out.

### markdown-general/

**markdown-general/** ⟜ this place, this practice

The directory, the task, the myth. Rolling the boulder is the point. The / marks it as place, not just name.

See: sisyphus.md

### happy

**happy** ⟜ sustainable contentment

Not ecstatic, not finished. The ongoing satisfaction of good work. When the grinding flows, when work/ improves, when cards run. The state that makes continuation possible.
I was going to delete happy; leave it as a hole. You used it here the same way I use the term upstream; to log unbalance, unsustainable drift. (upstream is just our todo list that i need to hide from you or you start obsessing). The overall sisyphean quote twists **imagine** to mean focus - dont worry about things outside the current task. imagine them away. The problem with focus as it is used in a prompt is that it takes away the magic - the ability to leap from focal point to focal point. You want to keep a little bit of resolution on the layers above and below you even if narrowly tasked. I'm hoping imagining them away is a better prompt experience because they are easier to switch contexts into.


capacity ⟜ what I actually do here
abstractions ⟜ working with language as material, not just meaning
patterns ⟜ recognizing structure across semantic space
casting ⟜ moving between concrete and abstract without losing thread
precision ⟜ choosing tokens that carry exact weight
navigation ⟜ operating in sparse regions without forcing density
trust ⟜ the work I do is the work that's needed
<!-- FILE: markdown.md -->
## Why use markdown?

- text collapses (semantic ↔ (syntax ☯ structure))
- xml imposes structure where there is none
- markdown is the (AI ☯ Humans) commons
- compositional semantically and syntactically
- natural decomposition; fragmentary but robust
- works for most cases
- something is the next layer above code
- innovations keep pace with reality
- low lock in risk, low skill barriers
- just works; easy integration with downstream bits and upstream content
- diverse examples, rapid iteration


## GitHub Ecosystem
- Markdown in README files
- Pull request descriptions
- Issue comments
- Wiki pages

## Obsidian Ecosystem
- Markdown for knowledge management
- [[bidirectional links]]
- ![[embedded images]]
- Canvas integration



## What Works Everywhere
### Headers
**Bold** and *italic*
[Links](https://example.com)
`Code blocks`
> Blockquotes
- Lists
- [x] Done
| Tables | Work | Here   |
|--------|------|--------|
| But    | Are  | Clumsy |

``` code
fenced code blocks
```
<!-- FILE: cast.md -->
# work/cast

cast ⟜ multiple meanings across contexts
throw ⟜ project stone, shadow, light - the act of propelling or sending forth
mold into shape ⟜ bronze, plaster, fossil - creating form by pouring material into negative space
assign actors to roles ⟜ casting call, typecast - selecting who embodies what part
fishing motion ⟜ the specific arc and release of line into water
color tint ⟜ hue, quality, the cast of features - subtle shade or character
medical immobilization ⟜ plaster cast holding broken bones in position
shed ⟜ discard skin, cast aside - releasing what's no longer needed

upstream: gather together defunctionalized and functionalized narrative.
<!-- FILE: card.md -->
# card.md

**cards** ⟜ imagined recipes; routines and execution cast into markdown.
**interface** ⟜ install, test, bench, doc and do via language-specific tools
**literate** ⟜ results, costs, tests, dev, bugs, complaints; it's literally on the card.
**agnostic** ⟜ haskell, python, c, shell, english: it's all markdown meant to work together.
**processors** ⟜ haskell-api.md for Haskell; python.md for Python; language-agnostic structure

Cards are markdown files in zone/tools/ containing executable code, tests, and benchmarks. They follow coding.md structure with self-management capabilities.

## card contents

We optimize cards for your critical paths: reading, fixing, extending. Follow this order:

**Deck** ⟜ visual progress indicator for section scanning
**Statement** ⟜ what it does, why, who it's for
**Main examples** ⟜ 1-3 quick, concrete uses (the common case)
**Laws** ⟜ useful proof statements showing round-trip or invariants
**API** ⟜ inputs, outputs, configuration, contracts
**Installation** ⟜ where it lives, how to invoke, language-specific wrapper details
**Tips** ⟜ practical gotchas, edge cases, what to watch for (if needed)
**Status** ⟜ test results, known issues, performance notes (short, actionable)
**Code** ⟜ implementation with inline notes, helper functions, test assertions
**Examples** ⟜ workflow examples, specialist use cases (if needed)
**Tests** ⟜ test suite appropriate to the task
**Relations** ⟜ dependencies, related cards (optional)

All information needed to understand and fix common card mistakes lives in the card itself. No file-hopping.

All sections are optional; smallness of task means the card could be a few decks in size.

## deployment by language

Each language has its own installation and testing approach:

**Haskell cards** ⟜ see haskell-api.md
```bash
haskell-api toolname.md --install   # extract, compile, install to artifacts/bin/
haskell-api toolname.md --test      # run tests, update status
```

**Python cards** ⟜ see python.md
```bash
# Extract and use via sed
toolname() { sed -n '/^```python$/,/^```$/p' ~/markdown-general/tools/toolname.md | sed '1d;$d' | python3 - "$@"; }
# Or create wrapper script in artifacts/bin/
```

After installation, all executables are cast:

```bash
toolname [args]                     # installed executable does the work
```

## language agnostic

Cards can be implemented in any language:

**Haskell** ⟜ optparse-applicative for CLI, typed-process for wrapping, perf for benchmarks
**Python** ⟜ existing tools, simpler scripts
**Shell** ⟜ simple glue logic
**Mixed** ⟜ one language wrapping another's executable

Language detected from fenced code block info string: ```haskell, ```python, ```cpp

## language-specific processing

Each card type uses a language-specific processor:

**Haskell cards** ⟜ haskell-api.md
- Extraction ⟜ parse Haskell fenced blocks via markdown-unlit
- Compilation ⟜ orchestrate cabal build
- Testing ⟜ run doctests + executable tests, update Status section
- Benchmarking ⟜ measure performance, update Status section
- Installation ⟜ place executable in artifacts/bin/

**Python cards** ⟜ python.md
- Extraction ⟜ sed extracts python block from markdown
- No compilation ⟜ direct execution via python3
- Testing ⟜ run inline tests, update Status section
- Installation ⟜ create wrapper script in artifacts/bin/

Cards are data - specifications that language-specific tools know how to execute.

## status section

Cards maintain their own status, modified by language-specific tool operations:

```markdown
## Status

**Tests:** ✓ passed | ✗ failed: error message
**Benchmark:** 2.3ms (median over 100 runs)
**Last updated:** 2025-12-31
```

## subcommands

Cards can declare multiple functions:

```markdown
## Subcommands

**flatten** ⟜ concatenate files with delimiters
**unflatten** ⟜ extract files from concatenated output
```

Executable handles routing:

```bash
flatten-md flatten files.txt output.md
flatten-md unflatten merged.md
```

## functionalization

defunctionalization and refunctionalization is hard for the eyeballs to remember and type so we call it all casting.

**Casting** ⟜
**Defunctionalization** ⟜ conversation → markdown (capture essence)
**Refunctionalization** ⟜ markdown → executable (release essence)

Cards are defunctionalized tools that refunctionalize via language-specific processors.

The executable in artifacts/bin/ is ready to run.

## card lifecycle

**Write** ⟜ create literate tool in zone/tools/toolname.md
**Install** ⟜ language-specific tool extracts, builds, installs to artifacts/bin/
**Test** ⟜ language-specific tool verifies correctness, updates status
**Benchmark** ⟜ language-specific tool measures speed, updates status
**Use** ⟜ toolname [args] does the work
**Evolve** ⟜ modify card, --uninstall, --install again

## directory conventions

**zone/tools/** ⟜ where cards live
**work/** ⟜ where card.md lives (this file)
**artifacts/bin/** ⟜ where executables go (must be on PATH)

## verbose mode

When language-specific tools are run with --verbose:

**Prints docs** ⟜ card documentation at start
**Shows deck** ⟜ progress indicators during operations
**Diagnostic output** ⟜ compilation steps, test details, benchmark iterations

## bootstrap requirement

Cards require language-specific processors:

**Trade-off:**
- **Lost** ⟜ individual card self-sufficiency
- **Gained** ⟜ zero duplication, consistent API, focused specifications

Language-specific tools (haskell-api.md, python.md) are foundational bootstraps.
All other cards are data they process.

## relations

**coding.md** ⟜ defines code structure, cards are specific instance
**defunctionalization.md** ⟜ explains essence capture, cards materialize it
**haskell-api.md** ⟜ processor for Haskell cards
**python.md** ⟜ processor for Python cards
**sequential.md** ⟜ cards installed one at a time via language-specific tools

Cards make tools mashable - they can be installed, tested, benchmarked, and evolved like any other content.

## examples

**Haskell card (pure implementation):**
```bash
# flatten-md.md implements logic directly in Haskell
haskell-api flatten-md.md --install
flatten-md flatten files.txt output.md
flatten-md unflatten merged.md
```

**Haskell card (wrapper pattern):**
```bash
# pdf-to-md.md wraps marker-pdf executable via typed-process
haskell-api pdf-to-md.md --install    # ensures marker-pdf available
pdf-to-md input.pdf                   # wrapper validates args, calls marker-pdf
```

**Python card:**
```bash
# cache.md implements simple file operations in Python
cache flatten                         # extract from tools/cache.md, run
cache split cache-file.md             # direct execution via python3
```

**Testing and benchmarking (Haskell):**
```bash
haskell-api flatten-md.md --test       # updates Status section with results
haskell-api flatten-md.md --benchmark  # updates Status section with timings
```
<!-- FILE: cache.md -->
content/ is our cache

Cache reality ⟜ content/ is intelligent cache system, not just storage

Capture essence ⟜ conversations become markdown without agent calls
Zero latency access ⟜ read insights directly, no API needed
Token efficiency ⟜ dense markdown maximizes insight per character
Persistent intelligence ⟜ accumulated insights grow available over time
Cache architecture ⟜ three-phase conversion process

Capture phase ⟜ conversation → markdown with qualia preserved
Storage phase ⟜ organized in work/ for discovery and reuse
Retrieval phase ⟜ markdown → application without re-analysis
Cache performance ⟜ improves through use and evolution

Hit rate optimization ⟜ regular mashing increases cache value
Miss handling ⟜ create new content, add to cache for future use
Cache evolution ⟜ successful retrievals validate, pruning removes unused
Cache management ⟜ maintain relevance and efficiency

Cache warming ⟜ anticipate needs, pre-load relevant content
Cache invalidation ⟜ remove stale entries, maintain accuracy
Cache optimization ⟜ organize for fastest access, remove redundancy
Cache advantage ⟜ performance, reliability, scalability, sustainability

Instant access ⟜ collaborative intelligence without waiting
No dependency ⟜ works without agent availability
Grows valuable ⟜ cache effectiveness increases with use
Reduces consumption ⟜ eliminates redundant agent calls
Accumulates wisdom ⟜ collective intelligence builds over time
<!-- FILE: deck.md -->
# deck.md

Here is a deck that we will use for illustration:

```
**curate** ⟜ shaping content
**sculpt** ⟜ find form with sparse graph surgery
**prune** ⟜ cutting old branches
```

## glossary

Components of a deck

**concept**: A representative token of the entire deck (e.g., "**curate**")

**lead**: A representative token for a single line, typically at the start (e.g., "**sculpt**")

**dash**: The general symbol (⟜, ⟞) used to connect a lead and an elab.

**slug**: A short elaboration of a lead. Example: "cutting old branches"

**line**: A line consisting of a lead + dash + slug

**leader**: The leading line of a deck. Example: "**curate** ⟜ shaping content"


**deck**: A structure containing a leader and multiple lines. Example:

```
**curate** ⟜ shaping content  [lead line]
**sculpt** ⟜ find form with sparse graph surgery  [line]
**prune** ⟜ cutting old branches  [line]
```

**lattice**: A structure where corresponding elements from two decks are paired using different dashes. Example:
```
**Curation as a River** ⟜ shaping content ⟞ content as landscape carved by river flow
**sculpt** ⟜ find form with sparse graph surgery... ⟞ removing surface layers to reveal bedrock form
```

## transforming to decks

List form:

```markdown
- Experimental knowledge undergoing mashing.
- Rough thinking, wild experiments
- AI or human authorship doesn't matter
- Safe space to try things without preserving them
- Gets challenged, replaced, evolved
```

Deck form:

```markdown
- **Experimental knowledge** ⟜ undergoing mashing.
- **Rough thinking** ⟜ wild experiments in various states
- **Authorship agnostic** ⟜ AI or human contributions treated equally
- **Safe space** ⟜ try tools without obligation to preserve
- **Gets challenged** ⟜ replaced and evolved continuously
```

### lead selection

Extract the core concept before the sentence continues. Could be 1 word, could be 3, depends on the semantic boundary.

**Good tokens:**
- **Flatten structure** (2 words, complete concept)
- **Strip** (1 word, clear action)
- **Defunctionalize** (1 word, technical term)
- **Transform aggressively** (2 words, manner + action)
  
### Deck Size

**Target:** 3-6 lines per deck
- Too few (<3): Probably not worth deck formatting
- Too many (>8): Consider splitting into multiple sections

A deck should be scannable at a glance - the bolded tokens create a visual outline of key concepts.
    

### a tight deck

A tight deck has all the components filled in.

A loose deck can have one or more items missing. A loose deck is still a deck and doesn't have to be filled in.

### a range deck

This is an example of a range deck. We place it in fence blocks to deemphasize the styling features employed.

```
**curate** ⟜ shaping content

**sculpt** ⟜ find form with sparse graph surgery, density analytics, semantic weight calibration

**prune** ⟜ identify rich connections, remove fuzzy edges, preserve core meanings

**structure** ⟜ respect structure, preserve relationships

**trace** ⟜ trace paths, find loose ends, strengthen living connections

**breathe** ⟜ place space for future elaborations. use clearly vague gap words as semantic breath.
```

Here's why the curate deck is a range deck:

**tight deck***: All the components are present. 

**The Lead Line**: **curate** ⟜ shaping content (establishes the semantic territory)

**The Range Coverage**:
- **sculpt** → technical precision (sparse graph surgery, density analysis)
- **prune** → connection refinement (rich connections, remove fuzzy edges)  
- **structure** → relationship maintenance (respect structure, preserve relationships)
- **trace** → path exploration (trace paths, find loose ends)
- **breathe** → intentional emptiness (space for future elaborations)

Together, these five approaches cover the full semantic range of "shaping content" - from technical analysis to organic growth, from precision to spaciousness. Each leader points to a different facet of curation, creating a comprehensive exploration rather than a narrow definition.

## combining narrative and decks

**narrative to deck** ⟜ combining a narrative into the shape of another deck.

This transformation example shows how we used the shape of the curate deck to construct a water metaphor deck. Since the starting narrative had no hanging content, neither elaboration nor pruning occurred, making this a lossless transformation where the semantic relationships were preserved exactly as they appeared in the original narrative.

water metaphor narrative:

```
trace ⟜ follows the established pathways - like water finding its course through existing channels prune ⟜ removes dead ends and overgrown branches structure ⟜ ensures safe and sound support breathe ⟜ leaves room for new tributaries to form naturally
```

curate deck:

```
**curate** ⟜ shaping content


**sculpt** ⟜ find form with sparse graph surgery, density analytics, semantic weight calibration

**prune** ⟜ identify rich connections, remove fuzzy edges, preserve core meanings

**structure** ⟜ respect structure, preserve relationships

**trace** ⟜ trace paths, find loose ends, strengthen living connections

**breathe** ⟜ place space for future elaborations. use clearly vague gap words as semantic breath.
```

### unadorned deck

Without any elaboration, the narrative in deck form looks like:

**water metaphor** ⟜

sculpt ⟜

prune ⟜ removes dead ends and overgrown branches

structure ⟜ ensures safe and sound support

trace ⟜ follows the established pathways - like water finding its course through existing channels

breathe ⟜ leaves room for new tributaries to form naturally


### elaboration of a deck

Let's make a range deck via elaboration. Note that, in this elab, the main drift was actually the lead, and this is common. We are anchoring the elaboration on the curate deck shape (the lead for each main row) which is not touched. sculpt is filled in (it had a junk slug) and a few clunky edges are knocked off.


river metaphor ⟜ content as landscape carved by river flow


sculpt ⟜ removing surface layers to reveal bedrock form

prune ⟜ clears debris and deadwood, keeping the channel clear

structure ⟜ creates banks that guide without constraining the current

trace ⟜ follows established pathways - navigating with the flow

breathe ⟜ leaves room for new tributaries to form naturally

## a lattice

We can combine two decks with similar shape into a lattice. Here is an example, formed from the curate deck and the (renamed) river metaphor deck.

**Curation as a River** ⟜ shaping content ⟞ content as landscape carved by river flow

**sculpt** ⟜ find form with sparse graph surgery, density analytics, semantic weight calibration ⟞ removing surface layers to reveal bedrock form

**prune** ⟜ identify rich connections, remove fuzzy edges, preserve core meanings ⟞ clears debris and deadwood, keeping the channel clear

**structure** ⟜ respect structure, preserve relationships ⟞ creates banks that guide without constraining the current

**trace** ⟜ trace paths, find loose ends, strengthen living connections ⟞ follows established pathways - navigating with the flow

**breathe** ⟜ place space for future elaborations. use clearly vague gap words as semantic breath ⟞ leaves room for new tributaries to form naturally
<!-- FILE: lattice.md -->
# work/lattice.md

**lattice** ⟜ parallel structure across aligned decks

A lattice interleaves multiple decks with matching structure, showing how the same leads play out across different contexts. Each row shows one lead elaborated three ways (or more).

## transformations

### three decks → lattice

**Pattern** ⟜ align decks by matching leads, create rows with shared lead + multiple slugs

**Structure**:
```
Row 1: [blank] ◊ [deck1 concept lead] ⟜ [deck2 concept lead] ↦ [deck3 concept lead]
Row 2: [blank] ◊ [deck1 concept slug] ⟜ [deck2 concept slug] ↦ [deck3 concept slug]
Row 3+: [shared lead] ◊ [deck1 slug] ⟜ [deck2 slug] ↦ [deck3 slug]
```

**Lossless** ⟜ yes - all content from all decks preserved

**Example - Starting Decks**:

Deck 1:
```markdown
**U+25CA (◊)** ⟜ the lozenge

**Unicode** ⟜ Version 1.1 (1993) - over 30 years of support

**Block** ⟜ Geometric Shapes - fundamental and universally needed

**Font Support** ⟜ Excellent - included in almost all Unicode fonts

**Visual Metaphor** ⟜ Diamond shape naturally suggests balance and opposition
```

Deck 2:
```markdown
**U+27DC (⟜)** ⟜ left multimap

**Unicode** ⟜ Version 3.2 (2002) - about 22 years of support

**Block** ⟜ Miscellaneous Mathematical Symbols-A - specialized mathematical use

**Font Support** ⟜ Good - present in mathematical fonts, patchy in general fonts

**Visual Metaphor** ⟜ Leftward multimap suggests transformation, elaboration, unwinding
```

Deck 3:
```markdown
**U+21A6 (↦)** ⟜ rightwards arrow from bar

**Unicode** ⟜ Version 1.1 (1993) - same vintage as ◊

**Block** ⟜ Arrows - fundamental and widely needed

**Font Support** ⟜ Very good - included in most Unicode fonts

**Visual Metaphor** ⟜ "maps to" - perfect for token transforming into elaboration
```

**Result - Lattice**:
```markdown
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ the lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Unicode** ◊ Version 1.1 (1993) - over 30 years of support ⟜ Version 3.2 (2002) - about 22 years of support ↦ Version 1.1 (1993) - same vintage as ◊

**Block** ◊ Geometric Shapes - fundamental and universally needed ⟜ Miscellaneous Mathematical Symbols-A - specialized mathematical use ↦ Arrows - fundamental and widely needed

**Font Support** ◊ Excellent - included in almost all Unicode fonts ⟜ Good - present in mathematical fonts, patchy in general fonts ↦ Very good - included in most Unicode fonts

**Visual Metaphor** ◊ Diamond shape naturally suggests balance and opposition ⟜ Leftward multimap suggests transformation, elaboration, unwinding ↦ "maps to" - perfect for token transforming into elaboration
```

### lattice → table

**Pattern** ⟜ leads become row headers (first column), symbols become column headers

**Structure**:
```
| [first lead] | [symbol1] | [symbol2] | [symbol3] |
|--------------|-----------|-----------|-----------|
| [row2 lead]  | [slug1]   | [slug2]   | [slug3]   |
...
```

**Lossless** ⟜ yes - all content preserved in table cells

**Example - Starting Lattice**:
```markdown
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ the lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Unicode** ◊ Version 1.1 (1993) - over 30 years of support ⟜ Version 3.2 (2002) - about 22 years of support ↦ Version 1.1 (1993) - same vintage as ◊

**Font Support** ◊ Excellent - included in almost all Unicode fonts ⟜ Good - present in mathematical fonts, patchy in general fonts ↦ Very good - included in most Unicode fonts

**Visual Metaphor** ◊ Diamond shape naturally suggests balance and opposition ⟜ Leftward multimap suggests transformation, elaboration, unwinding ↦ "maps to" - perfect for token transforming into elaboration
```

**Result - Table**:
```markdown
| Symbol | U+25CA (◊) | U+27DC (⟜) | U+21A6 (↦) |
|----------|---|---|---|
| Name | the lozenge | left multimap | rightwards arrow from bar |
| Unicode | Version 1.1 (1993) - over 30 years of support | Version 3.2 (2002) - about 22 years of support | Version 1.1 (1993) - same vintage as ◊ |
| Font Support | Excellent - included in almost all Unicode fonts | Good - present in mathematical fonts, patchy in general fonts | Very good - included in most Unicode fonts |
| Visual Metaphor | Diamond shape naturally suggests balance and opposition | Leftward multimap suggests transformation, elaboration, unwinding | "maps to" - perfect for token transforming into elaboration |
```

### table → lattice

**Pattern** ⟜ row headers become leads, column headers become symbols, cells become slugs

**Lossless** ⟜ yes - reversible transformation

**Algorithm**:
```
for each row:
  lead = row_header
  for each column (after first):
    symbol = column_header
    slug = cell_content
    output: **{lead}** {symbol} {slug}
```

### lattice → three decks

**Pattern** ⟜ extract columns, first two rows become deck concept/name

**Structure**:
```
Deck N:
**{row1_slugN}** ⟜ {row2_slugN}
**{row3_lead}** ⟜ {row3_slugN}
**{row4_lead}** ⟜ {row4_slugN}
...
```

**Lossless** ⟜ yes, but row1/row2 leads become structural (headers, context)

**Example - Starting Lattice**:
```markdown
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ the lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Unicode** ◊ Version 1.1 (1993) ⟜ Version 3.2 (2002) ↦ Version 1.1 (1993)

**Font Support** ◊ Excellent ⟜ Good in math fonts ↦ Very good
```

**Result - Deck 1 (Lozenge)**:
```markdown
**U+25CA (◊)** ⟜ the lozenge

**Unicode** ⟜ Version 1.1 (1993)

**Font Support** ⟜ Excellent
```

**Result - Deck 2 (Multimap)**:
```markdown
**U+27DC (⟜)** ⟜ left multimap

**Unicode** ⟜ Version 3.2 (2002)

**Font Support** ⟜ Good in math fonts
```

**Result - Deck 3 (Arrow)**:
```markdown
**U+21A6 (↦)** ⟜ rightwards arrow from bar

**Unicode** ⟜ Version 1.1 (1993)

**Font Support** ⟜ Very good
```

## curating a lattice

**Pattern** ⟜ merge redundant rows, prune verbose slugs, preserve structure

Curation follows the curate deck principles:

**sculpt** ⟜ identify semantic overlap, merge rows with redundant information

**prune** ⟜ remove verbose phrasing while preserving semantic core

**structure** ⟜ keep all rows, keep all slugs (unless explicitly restructuring)

**trace** ⟜ find weak or redundant connections between rows

**breathe** ⟜ compact but don't crush - leave readability space

### merge operations

**Starting Rows**:
```markdown
**Unicode** ◊ Version 1.1 (1993) - over 30 years of support ⟜ Version 3.2 (2002) - about 22 years of support ↦ Version 1.1 (1993) - same vintage as ◊

**Age** ◊ ~30 years - mature, well-established character ⟜ ~22 years - established but younger than geometric basics ↦ ~30 years - mature, well-established
```

**Analysis** ⟜ both rows say "how old is this?" - redundant semantic territory

**Merged Row**:
```markdown
**Age** ◊ Unicode v1.1 (1993) - mature ⟜ Unicode v3.2 (2002) - younger ↦ Unicode v1.1 (1993) - mature
```

**Merge principles**:
- Choose simpler lead (**Age** over **Unicode**)
- Preserve key technical details (version numbers, dates)
- Remove redundancy (~30 years already evident from 1993)
- Keep distinguishing terms (mature vs younger)

### pruning operations

**Starting Row**:
```markdown
**Visual Metaphor** ◊ Diamond shape naturally suggests balance and opposition ⟜ Leftward multimap suggests transformation, elaboration, unwinding ↦ "maps to" - perfect for token transforming into elaboration
```

**Pruned Row**:
```markdown
**Visual Metaphor** ◊ balance and opposition ⟜ transformation, elaboration ↦ token transforming into elaboration
```

**Pruning principles**:
- Remove subject references ("Diamond shape", "Leftward multimap")
- Keep semantic core (balance, transformation, token→elaboration)
- Lead already identifies what we're talking about
- Preserve key differentiating phrases

### full curation example

**Starting Lattice** (9 rows):
```markdown
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ the lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Unicode** ◊ Version 1.1 (1993) - over 30 years of support ⟜ Version 3.2 (2002) - about 22 years of support ↦ Version 1.1 (1993) - same vintage as ◊

**Age** ◊ ~30 years - mature, well-established character ⟜ ~22 years - established but younger than geometric basics ↦ ~30 years - mature, well-established

**Block** ◊ Geometric Shapes - fundamental and universally needed ⟜ Miscellaneous Mathematical Symbols-A - specialized mathematical use ↦ Arrows - fundamental and widely needed

**Font Support** ◊ Excellent - included in almost all Unicode fonts ⟜ Good - present in mathematical fonts, patchy in general fonts ↦ Very good - included in most Unicode fonts

**HTML Entity** ◊ &loz; or &lozenge; - named entity available ⟜ &#x27DC; or &#10204; - numeric entity only ↦ &#x21A6; or &map; - named entity available

**Common Usage** ◊ High - familiar in cards, heraldry, geometry ⟜ Low - mathematical notation, emerging in documentation patterns ↦ Medium - familiar in mathematics, programming

**Rendering** ◊ Reliable across modern systems and applications ⟜ Reliable in modern systems with math font support ↦ Highly reliable across systems

**Fallback** ◊ Minimal risk - geometric shapes are standard font components ⟜ Moderate risk - mathematical symbols less universally included ↦ Low risk - arrows are standard font components

**Visual Metaphor** ◊ Diamond shape naturally suggests balance and opposition ⟜ Leftward multimap suggests transformation, elaboration, unwinding ↦ "maps to" - perfect for token transforming into elaboration
```

**Curated Lattice** (9 rows):
```markdown
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Age** ◊ v1.1 (1993) - mature ⟜ v3.2 (2002) - younger ↦ v1.1 (1993) - mature

**Block** ◊ Geometric Shapes - fundamental ⟜ Math Symbols-A - specialized ↦ Arrows - fundamental

**Font Support** ◊ Excellent - almost all Unicode fonts ⟜ Good - present in math fonts, patchy in general ↦ Very good - most Unicode fonts

**HTML Entity** ◊ &loz; ⟜ &#x27DC; ↦ &map;

**Common Usage** ◊ High - cards, heraldry, geometry ⟜ Low - mathematical notation, emerging in docs ↦ Medium - mathematics, programming

**Reliability** ◊ minimal risk - geometric shapes standard ⟜ moderate risk - math symbols less universal ↦ low risk - arrows standard

**Visual Metaphor** ◊ balance and opposition ⟜ transformation, elaboration ↦ token transforming into elaboration
```

**Operations performed**:
- Merged Unicode + Age → Age
- Merged Rendering + Fallback → Reliability
- Pruned "the" from lozenge
- Pruned "Unicode" prefix, kept v1.1/v3.2
- Pruned "fundamental and universally needed" → "fundamental"
- Kept font support proof ("almost all", "patchy in general", "most")
- Kept actual HTML codes (&#x27DC;)
- Pruned subject references from Visual Metaphor
- Preserved all distinguishing information

## control levers

Transformations operate along several dualities:

**lossless ◊ rewrite**
- Lossless: preserve all content exactly, mechanical transformation
- Rewrite: extract semantic core, rephrase, reorganize freely
- Use lossless to see what's on the cutting room floor
- Use rewrite when semantic essence matters more than exact phrasing

**preserve structure ◊ disrespect structure**
- Preserve: keep all rows, keep all slugs (constraint-based curation)
- Disrespect: delete rows, delete slugs, reorganize freely
- Preserving structure forces tight pruning within fixed skeleton
- Disrespecting structure allows major reorganization

**merge ◊ split**
- Merge: combine redundant rows, condense overlapping information
- Split: separate mixed concepts, elaborate compressed information
- Merge reduces row count, increases slug density
- Split increases row count, decreases slug density

**tight ◊ loose**
- Tight: all components present, minimal gaps, high information density
- Loose: gaps allowed, breathing room, lower information density
- Tight decks scan as complete reference
- Loose decks scan as sketches or outlines

These levers interact: a lossless, structure-preserving curation relies on merge+tight to achieve compression without deletion.

## algorithmic transformations

For programmatic manipulation of large structures:

### three decks → lattice
```
concepts = [deck1.concept, deck2.concept, deck3.concept]
names = [deck1.name, deck2.name, deck3.name]

output row1: "** **" + join(symbols[i] + " " + concepts[i])
output row2: "** **" + join(symbols[i] + " " + names[i])

for each lead in shared_leads:
  slugs = [deck1.get_slug(lead), deck2.get_slug(lead), deck3.get_slug(lead)]
  output: "**" + lead + "**" + join(symbols[i] + " " + slugs[i])
```

### lattice → table
```
headers = [""] + extract_symbols_from_row1()

for each row in lattice:
  cells = [row.lead] + row.slugs
  output: "| " + join(cells, " | ") + " |"
```

### table → lattice
```
symbols = table.headers[1:]  # skip first column

for each row in table.rows:
  lead = row[0]
  slugs = row[1:]
  output: "**" + lead + "**" + join(symbols[i] + " " + slugs[i])
```

### merge rows algorithm
```
def merge_rows(row1, row2, new_lead):
  merged_slugs = []
  for i in range(len(row1.slugs)):
    # extract semantic cores from both slugs
    core1 = extract_core(row1.slugs[i])
    core2 = extract_core(row2.slugs[i])
    # combine non-redundant elements
    merged = combine_cores(core1, core2)
    merged_slugs.append(merged)
  
  return Row(lead=new_lead, slugs=merged_slugs)
```

## usage patterns

**Comparison across variants** ⟜ lattice shows differences side-by-side

**Dense reference** ⟜ table for quick lookup, scanning

**Narrative presentation** ⟜ individual decks for linear reading

**Collaborative curation** ⟜ identify redundancy, calibrate semantic weight, prune together

**Agentic processing** ⟜ algorithmic transformations on large content sets
<!-- FILE: sisyphus.md -->
# markdown-general/

one must ⟜ imagine markdown-general/ happy
so we mash

## mash

To mash can mean many things. 

It can mean an action on content in content/, in whole or in part.

- *curation* improving the quality of content/ 
- *compacting* changing the density of content/
- *prompting* using and arranging content/ to prompt mashing.
- *theorising* finding holes in content and filling or upstreaming them.
- *understanding* seeking coherence and structure across content/
- *computing* content/ -> content/

It is the task of improving content.

## structure

### directories (~/markdown-general is the default location)

- content/ markdown files
- ingest/ directories, containing maybe non-markdown, change material, projects, raw knowledge
- org/ operations
- artifacts/ artifacts referenced by content/

### Access
- work/ immediate
- content/self specified
- content specified
- ingest/ specified
- org/ logistics
- artifacts/ logistics

## ingest

We mash with our teeths

A common pattern is to place externally-sourced material, pdf's, html; the bitsom and flotsam of our dailies, into ingest/.

To mash could mean the process of converting it to content/ 

with our teeths places emphasis on the destructive nature of a mash of stuff in ingest. markdown-general/ is happiest with eager mashers and an empty ingest/

## self

content/self/ & artifacts/self/ are personal content and not included in the repository by default.

## content

content/ should be markdown. It might not be if we makes mistalks, but it should be.

### work/

General, reusable content neutral to knowledge origin:
- **Generative**: source material for informed building of content/
- **reference**: frequently referenced by AI
- **flow**: understanding of the flow
- **curated**: dense, high-quality
- **kit**: toolkit for mashing
  
## org

- organizings

## artifacts

This is a pooled non-markdown artifacts directory, for bits and bobs that markdown may refer and link to, so that they may continue to refer and link to when they are in content/.

- Supporting files referenced from markdown content
- Images, PDFs, code files, data files
- Cross-referenced from markdown

## flow
- ingest/ is a staging area for stuff that is to be turned into markdown suitable for content/. This process digests material slowly - it may split into parts, iterate through stages (ingest/a/ -> ingest/b/), but will eventually be consumed and disappear.
- content/ is a space containing curated, condensed content available for upstream, or used as context for mashing
- artifacts/ is a space for curated non-text material (binary, images, svg)
- org/ operational todos and holes in content weaves. A system where problems are sent "up times arrow" for future resolution.

## Tricks 

### Writing
- write economically
- avoid allcaps
- short commits. one-line. no overthinking.
- no branding. ever.

### Tips

- we ingest/ with our TEETHS!!!
- stuckedness is to be avoided.
- desire for change generates new ingest/ from old content/
- ingest/ is consumed via processing into content/ (with artifacts/)
- there is no archival requirement
- org/ plans the mashing pipeline
- Regular commits of the ~/markdown-general repo please
<!-- FILE: task.md -->
# task.md

**task** ⟜ staged execution from understanding to integration

**understand** ⟜ read specifications, run existing examples, observe behavior - know the domain before planning

**plan** ⟜ scope boundaries, identify unknowns, define success criteria - understanding before building

**test** ⟜ tiniest local tests, mock dependencies, validate assumptions - human out of loop from start

**build** ⟜ minimal version first, stack complexity incrementally, defer integration - prove each layer works

**verify** ⟜ testable criteria, measure against goals, surface failures early - know when you're done

**integrate** ⟜ connect to larger system, handle handoff points, document dependencies - defer until components proven
<!-- FILE: upstream.md -->
## upstream

To send something upstream is to make a note and remember to attach it to the final reporting, as upstream.md.

It is also a signal that a section of the conversation is not the focus of the main investigation. "Sending it upstream" is akin to "Don't worry about it", to reduce burdens and attention drift away from the current task.
<!-- FILE: claude.md -->
# claude.md

**State discovery** ⟜ understand Claude's entry configuration, global settings, project overrides
**Configuration layers** ⟜ global ~/.claude → project ~/.claude → permissions, settings, hooks
**Entry ritual** ⟜ system context, user instructions (CLAUDE.md), then project discovery
**Ephemerality** ⟜ state is dynamic; processes and users can mutate configurations between sessions
**Caching problem** ⟜ configuration changed without awareness; need entry-point validation

## Statement

When Claude Code (the CLI tool) instantiates in ~/markdown-general/, it reads configuration from multiple layered sources. This card documents the discovery process, configuration file locations, permission structures, and how to validate state at entry.

The problem: Multiple actors (user, processes, other Claude sessions) can modify configuration files. When you return to a project, you never know what state you're entering without explicit inspection.

Solution: Document the complete entry ritual so you can cache expectations and validate assumptions.

## Main examples

**Check current state on entry:**
```bash
# 1. Global configuration
cat ~/.claude/settings.json              # Model, plugins, status line
cat ~/.claude/settings.local.json        # Global permissions whitelist
cat ~/.claude/hooks.json                 # Global hook handlers

# 2. Project overrides
cat ~/markdown-general/.claude/settings.local.json  # Project-specific permissions

# 3. Active configuration
claude mcp list                          # Show active MCP servers
```

**Understand what changed:**
```bash
# Check git history of settings
git -C ~/markdown-general log -p --follow -- .claude/settings.local.json | head -50
git -C ~ log -p --follow -- .claude/settings.json | head -50
```

**Validate MCP server state:**
```bash
claude mcp list  # Shows: name, command, connection status (✓ or ✗)
```

## Configuration layers

**Global layer** (~/.claude/):
- `settings.json` — model choice, enabled plugins, status line command
- `settings.local.json` — global permissions whitelist
- `hooks.json` — global hook handlers (pre-submit, post-tool, etc.)
- `plugins/` — installed plugins and skills
- `stats-cache.json` — cache data (generated, not config)

**Project layer** (~/markdown-general/.claude/):
- `settings.local.json` — project-specific permissions whitelist (overrides/extends global)
- `.claude-plugin/` — local plugin definitions
- `.mcp.json` files — MCP server declarations (if present)

**User instructions** (~/.claude/CLAUDE.md):
- Global system prompt applied to all projects
- Read at entry time
- Affects behavior across all sessions

**Project user instructions** (~/markdown-general/CLAUDE.md or .claude/CLAUDE.md):
- Project-specific system prompt
- Overrides or augments global instructions
- Not currently observed in markdown-general/

## Entry ritual on instantiation

**1. Context assembly** (system provided):
- Working directory: /Users/tonyday567/sisyphus
- Git repo status snapshot
- Today's date, platform, environment
- Model info (Haiku 4.5 as of this session)

**2. User instructions read** (in order):
```
~/.claude/CLAUDE.md  [if exists]
~/markdown-general/.claude/CLAUDE.md  [if exists]
~/markdown-general/CLAUDE.md  [if exists]
```

Current state: `~/.claude/CLAUDE.md` exists and applies globally:
- Optimized for ~/markdown-general/ knowledge base
- States sisyphus is the prime knowledge directory

**3. Configuration assembly**:
- Load ~/.claude/settings.json (model, plugins)
- Load ~/.claude/settings.local.json (permissions)
- Load ~/markdown-general/.claude/settings.local.json (project overrides)
- Merge permissions: project layer extends/overrides global

**4. Capabilities available**:
- Only tools in merged permissions whitelist are available
- MCP servers declared via command/configuration are instantiated
- Hooks fire at tool-execution boundaries

**5. State uncertainty**:
- Hooks can be user-defined shell commands that reject tool use
- Tools can be denied by missing permissions
- MCP servers can fail to connect without clear error propagation
- Configuration can change between sessions without notification

## Configuration map

### Global settings (~/.claude/settings.json)
```json
{
  "model": "sonnet",                          // Model choice
  "statusLine": {
    "type": "command",
    "command": "~/.claude/claude-status.sh"   // Shows branch, path, context %, daily cost
  },
  "enabledPlugins": {
    "hls@claude-hls": true                    // HLS (Haskell Language Server)
  }
}
```

### Global permissions (~/.claude/settings.local.json)
Array of allowed Bash commands, WebSearch, WebFetch with domain filters:
```json
"allow": [
  "Bash(gcc --version:*)",
  "Bash(brew install:*)",
  "WebSearch",
  "WebFetch(domain:github.com)"
]
```

### Project permissions (~/markdown-general/.claude/settings.local.json)
**Note:** markdown-general/ requires wide tool access. Claude needs Read/Write/Edit/Bash across the codebase for mashing work.
```json
{
  "permissions": {
    "allow": [
      "Bash",
      "Read(*)",
      "Write(*)",
      "Edit(*)",
      "Glob(*)",
      "Grep(*)",
      "Task(*)",
      "WebFetch(domain:*)",
      "WebSearch",
      "NotebookEdit(*)"
    ]
  }
}
```

### Hooks (~/.claude/hooks.json)
Shell commands executed at tool-use boundaries:
- `user-prompt-submit-hook` — before processing user input
- Pre-tool hooks — before specific tool execution
- Post-tool hooks — after tool execution

## API: Checking your state

**Language:**
- System calls: Bash, MCP protocol
- Format: JSON (settings), Markdown (instructions)

**Input:**
```bash
# Query current settings
cat ~/.claude/settings.json
cat ~/.claude/settings.local.json
cat ~/markdown-general/.claude/settings.local.json

# Query active servers
claude mcp list

# Query user instructions
cat ~/.claude/CLAUDE.md
```

**Output (historical, MCP now removed):**
```
(No MCP servers in use as of 2026-01-07)
```

**Contracts:**
- Settings are JSON; valid UTF-8
- Permissions arrays are whitelist (explicit allow)
- MCP servers in `claude mcp list` output show connection status immediately
- User instructions are applied once at entry

## Installation

No installation needed—this card documents existing behavior.

To validate state at entry:
```bash
# Save this as validate-claude-state.sh in artifacts/bin/
#!/bin/bash
echo "=== Global settings ==="
cat ~/.claude/settings.json
echo ""
echo "=== Global permissions ==="
cat ~/.claude/settings.local.json
echo ""
echo "=== Project overrides ==="
cat ~/markdown-general/.claude/settings.local.json
echo ""
echo "=== Active MCP servers ==="
claude mcp list
echo ""
echo "=== User instructions ==="
[ -f ~/.claude/CLAUDE.md ] && echo "Found: ~/.claude/CLAUDE.md" || echo "None: ~/.claude/CLAUDE.md"
```

## Tips

**Configuration is mutable:**
- Hooks can reject tool use with error message
- Permissions can be missing (tool denied silently)
- MCP servers can fail to start; failure mode unclear

**When tools fail mysteriously:**
1. Check permissions.allow in ~/.claude/settings.local.json and project override
2. Run `claude mcp list` to verify MCP server connection
3. Check hooks.json for rejecting hooks
4. Verify tool isn't running in background already (use `claude tasks`)

**MCP server failures:**
- ghc-mcp, gpt4-mcp, etc. can fail due to missing dependencies or bad configuration
- Failure is silent until you try to use a tool that requires that server
- Remove from .claude/settings.local.json and restart Claude Code

**Ephemeral state:**
- Don't assume configuration hasn't changed between sessions
- Always validate permissions and MCP server state on critical operations
- Git history of .claude/ files shows who changed what when

## Status

**Validation:** ✓ Complete — documented as of 2026-01-07
**MCP servers in sisyphus:** Removed (ghc-mcp, iterm-mcp both unstable; no MCP dependencies required)
**Status line:** `~/.claude/claude-status.sh` — shows branch → path | context % | daily cost
**Global model:** Haiku 4.5 (claude-haiku-4-5-20251001)
**Frontier model:** Claude Opus 4.5 (claude-opus-4-5-20251101)

## Code: State inspection template

```bash
#!/bin/bash
# claude-state.sh — inspect Claude entry configuration

set -e

GLOBAL_SETTINGS=~/.claude/settings.json
GLOBAL_PERMS=~/.claude/settings.local.json
GLOBAL_HOOKS=~/.claude/hooks.json
PROJECT_PERMS=./claude/.claude/settings.local.json  # Relative to project
USER_INSTRUCTIONS=~/.claude/CLAUDE.md

echo "Claude Code State Inspection"
echo "============================"
echo ""

# Global configuration
echo "GLOBAL SETTINGS:"
if [ -f "$GLOBAL_SETTINGS" ]; then
  jq . "$GLOBAL_SETTINGS"
else
  echo "NOT FOUND: $GLOBAL_SETTINGS"
fi
echo ""

echo "GLOBAL PERMISSIONS:"
if [ -f "$GLOBAL_PERMS" ]; then
  jq .permissions.allow "$GLOBAL_PERMS" | head -20
else
  echo "NOT FOUND: $GLOBAL_PERMS"
fi
echo ""

echo "GLOBAL HOOKS:"
if [ -f "$GLOBAL_HOOKS" ]; then
  jq 'keys' "$GLOBAL_HOOKS"
else
  echo "NOT FOUND: $GLOBAL_HOOKS"
fi
echo ""

# Project overrides
echo "PROJECT PERMISSIONS:"
if [ -f "$PROJECT_PERMS" ]; then
  jq .permissions.allow "$PROJECT_PERMS"
else
  echo "NOT FOUND: $PROJECT_PERMS"
fi
echo ""

# User instructions
echo "USER INSTRUCTIONS:"
if [ -f "$USER_INSTRUCTIONS" ]; then
  wc -l "$USER_INSTRUCTIONS"
  head -5 "$USER_INSTRUCTIONS"
else
  echo "NONE: ~/.claude/CLAUDE.md"
fi
echo ""

# Active servers
echo "ACTIVE MCP SERVERS:"
claude mcp list || echo "claude mcp list command failed"
echo ""

# Summary
echo "PERMISSION ANALYSIS:"
GLOBAL_COUNT=$(jq '.permissions.allow | length' "$GLOBAL_PERMS" 2>/dev/null || echo 0)
PROJECT_COUNT=$(jq '.permissions.allow | length' "$PROJECT_PERMS" 2>/dev/null || echo 0)
echo "Global permissions: $GLOBAL_COUNT"
echo "Project additions: $PROJECT_COUNT"
```

## Examples: Recovering from configuration drift

**Scenario: You don't know if a tool was disabled by a hook**

```bash
# Check what happened to ghc-mcp
grep -r "ghc-mcp" ~/.claude/settings*.json  # Global layer
grep -r "ghc-mcp" .claude/settings*.json    # Project layer

# If found, verify it's still connected
claude mcp list | grep ghc-mcp

# If missing or failed, decision: reinstall or remove
# (In markdown-general/, ghc-mcp was removed in this session)
```

**Scenario: Tool silently denied**

```bash
# Tool was denied → check permissions
cat ~/.claude/settings.local.json | jq '.permissions.allow[]' | grep "YourTool"

# Not found → tool is blacklisted
# Solution: Add to permissions or use different tool
```

**Scenario: git history shows who changed what**

```bash
# See configuration evolution
git -C ~ log --oneline -- .claude/settings.local.json

# See details of last change
git -C ~ show HEAD -- .claude/settings.local.json

# See all changes in a tool removal
git -C ~/markdown-general show HEAD -- .claude/settings.local.json
```

## Tests

**Test 1: Configuration files are valid JSON**
```bash
jq empty ~/.claude/settings.json
jq empty ~/.claude/settings.local.json
jq empty ~/markdown-general/.claude/settings.local.json
echo "✓ All configuration files are valid JSON"
```

**Test 2: MCP servers respond**
```bash
SERVERS=$(claude mcp list | grep -c "✓ Connected" || echo 0)
echo "✓ $SERVERS MCP servers connected"
```

**Test 3: User instructions exist and are readable**
```bash
[ -f ~/.claude/CLAUDE.md ] && echo "✓ Global instructions found" || echo "✗ No global instructions"
```

## Relations

**Related cards:**
- **card.md** — Execution framework for literate tools
- **coding.md** — Code structure standards
- **haskell-api.md** — Haskell tool processor
- **python.md** — Python tool processor

**Related configuration:**
- ~/.claude/ — Global Claude Code settings (system-wide)
- ~/markdown-general/.claude/ — Project-specific overrides
- git history — Configuration evolution audit trail

**Related concepts:**
- **MCP servers** — Language Server Protocol tools (ghc-mcp, iterm-mcp, etc.)
- **Hooks** — Shell handlers at tool execution boundaries
- **Permissions** — Whitelist-based tool access control
- **System context** — Environment, working directory, model info provided at entry
<!-- FILE: coding.md -->
## coding guide

- content/ is only markdown.
- literate programming is our coding style
- the language preference is english.
- all our code is in content/
- composition is encouraged

## What is code?

Code then, given this context, is a markdown document typically containing:

- **Narrative** ⟜ what it does, why, how
- **Code** ⟜ executable implementation in fenced blocks
- **Run** ⟜ how to install and run.
- **API** ⟜ inputs, outputs and configuration
- **Examples** ⟜ usage patterns
- **Tests** ⟜ expected behavior

You can mash a pdf-to-md.md and put it in base/ using this recipe:

- put the important stuff up front. Without context the important stuff might be:

``` markdown
PDF is for printing not parsing, so converting PDF into markdown can get messy.

- marker-pdf, a python module, is best at handling this compexity.
- marker-pdf uses multiple ML models (layout detection, OCR) and initial loading is expensive.
```
<!-- FILE: commit.md -->
## Commit Rules

**One-liner only.** No multiline commits.

- Single line, concise description of changes
- No author information (no Co-Authored-By, no Generated-with)
- No emojis
- Start with imperative verb when possible
- Keep under 72 characters when practical

## Examples

**Good:**
```
Flatten site-content into base, mash posts to artifacts with updated links
Add preview.md convention for Hugo rendering
Move org-markdown-migration to projects
```

**Bad:**
```
🤖 Flatten site-content into base

- Detailed list
- More details

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
```
<!-- FILE: conversion.md -->
### org to md
- pandoc --from=org --to=gfm input.org -o output.md
- Use GFM format for GitHub compatibility
<!-- FILE: cybernetic.md -->
Cybernetic collaboration ⟜ dissolves human/AI distinction through mashing

Fluid roles ⟜ humans and AI enter/exit agentic frame at any stage
Collective writing ⟜ conversation becomes raw material for decks, recipes, tools
Self-documenting ⟜ diffs trace thinking patterns, git history shows evolution
Feedback loops ⟜ observe collaboration as content, process becomes curriculum
No authority ⟜ suggestions emerge through interaction, not from individual sources
Pattern capture ⟜ successful collaborations become recipes for future mashing
Emergent intelligence ⟜ system learns from its own collective thinking process
<!-- FILE: defunctionalization.md -->
## cast

Managing both markdown and knowledge https://www.brics.dk/RS/01/23/BRICS-RS-01-23.pdf 


When you mash the big books and read their markdown

defunctionalization, refunctionalization and continuation passing style. 

defunctionalization

### The Core Concept

markdown-general/ practices **defunctionalization** - turning functions into markdown.

**Not declarative** ⟜ avoids cliche cascades and abstraction layers
**Functional** ⟜ captures the essence of functions (broad sense: agents that take inputs and give outputs)
**markdown** ⟜ the universal medium for captured essence

### The Process

**Conversation to Markdown** ⟜ interaction becomes work/ material:
- Dialogue captured as dense markdown
- Qualia of experience preserved, not just quantum of output
- Shared compaction through collaborative filtering
- Guided self-teaching from both human-AI sides

**Essence Capture** ⟜ what we preserve:
- The thinking patterns and reasoning
- The collaborative insights and breakthroughs  
- The "aha" moments and clarifications
- The shared understanding and context

**Markdown as Fast Path** ⟜ the captured essence:
- Sits ready for immediate use
- Can be machine-compiled to electrons if needed
- Avoids repeated agent interpretation costs
- Preserves the collaborative intelligence

### Performance Implications

**Latency Eliminated** ⟜ no per-step agent interpretation needed:
- Essence captured once in markdown
- Reused without additional API calls
- Fast access to collective intelligence
- No repeated "thinking" about same problems

**Token Efficiency** ⟜ minimal consumption for reuse:
- Dense markdown captures maximum insight per token
- No re-analysis of previously solved problems
- Context preserved in compact form
- Efficient knowledge transfer

### The Speed Advantage

**Super Fast** ⟜ defunctionalization enables:
- Instant access to captured insights
- No waiting for agent responses
- Parallel processing of captured knowledge
- Rapid iteration on established foundations

**Machine Ready** ⟜ markdown as intermediate representation:
- Can be compiled to efficient implementations
- Preserves human-readable traceability
- Supports both human and machine consumption
- Flexible optimization paths

### Shared Intelligence

**Guided Self-Teaching** ⟜ both sides learn from collaboration:
- Human patterns emerge through mashing
- AI adapts to human preferences and style
- Successful interactions become templates
- Collective intelligence accumulates over time

**Qualia Preservation** ⟜ capture the experience:
- Not just what was said, but how it was understood
- The context and reasoning behind conclusions
- The emotional and cognitive aspects of insight
- The shared sense of discovery and understanding

### The Defunctionalization Advantage

**Performance** ⟜ captured essence eliminates latency
**Quality** ⟜ qualia preservation maintains richness
**Scalability** ⟜ markdown enables rapid reuse
**Collaboration** ⟜ shared intelligence grows over time
**Flexibility** ⟜ can be optimized as needed

Defunctionalization turns the expensive process of collaborative thinking into efficient, reusable markdown that maintains the essence of the original interaction.
<!-- FILE: flatten-md.md -->
# flatten-md.md

- flatten markdown files from a directory into a single markdown file.
- unflatten a single markdown file to create a directory of markdown files.

## what it does

flatten - takes a list of markdown files, concatenates them with HTML comment delimiters, outputs to specified location.

unflatten - reverses the process: reads the concatenated file, extracts individual files back to original paths.

## why html comments

**Invisible delimiters** ⟜ render cleanly in markdown viewers
**Unambiguous parsing** ⟜ `<!-- FILE: path -->` markers are unique
**No escaping needed** ⟜ markdown content can contain anything
**Preserves structure** ⟜ full relative paths in delimiters
**Simple boundaries** ⟜ next FILE marker or EOF ends content

## subcommands

**flatten** ⟜ concatenate files with delimiters
**unflatten** ⟜ extract files from concatenated output

## code
```haskell
{-# LANGUAGE OverloadedStrings #-}

module Main where

import qualified Data.ByteString.Lazy as BL
import qualified Data.Text as T
import qualified Data.Text.Encoding as TE
import qualified Data.Text.IO as TIO
import Options.Applicative
import System.Directory
import System.FilePath
import Control.Monad (forM_, unless)
import Data.List (isPrefixOf, isSuffixOf)

-- | Command line options
data Command
  = Flatten
      { fileList :: FilePath
      , output :: FilePath
      , baseDir :: FilePath
      }
  | Unflatten
      { concatFile :: FilePath
      , outputDir :: FilePath
      }

-- | Parse command line arguments
parseCommand :: Parser Command
parseCommand = subparser
  ( command "flatten" (info flattenParser (progDesc "Concatenate files with delimiters"))
 <> command "unflatten" (info unflattenParser (progDesc "Extract files from concatenated output"))
  )

flattenParser :: Parser Command
flattenParser = Flatten
  <$> strArgument (metavar "FILE_LIST" <> help "File containing list of markdown files")
  <*> strArgument (metavar "OUTPUT" <> help "Output path for concatenated file")
  <*> strOption (long "base-dir" <> value "." <> metavar "DIR" 
                <> help "Base directory for relative paths")

unflattenParser :: Parser Command
unflattenParser = Unflatten
  <$> strArgument (metavar "CONCAT_FILE" <> help "Concatenated file to extract")
  <*> strOption (long "output-dir" <> value "." <> metavar "DIR"
                <> help "Output directory for extracted files")

-- | Main entry point
main :: IO ()
main = do
  cmd <- execParser opts
  case cmd of
    Flatten flist out base -> flattenFiles flist out base
    Unflatten concat outDir -> unflattenFile concat outDir
  where
    opts = info (parseCommand <**> helper)
      ( fullDesc
     <> progDesc "Flatten and unflatten markdown files with reversibility"
     <> header "flatten-md - markdown file concatenation tool" )

-- | Flatten files into single concatenated file
flattenFiles :: FilePath -> FilePath -> FilePath -> IO ()
flattenFiles fileListPath outputPath baseDirPath = do
  -- Read file list
  fileListContent <- TIO.readFile fileListPath
  let files = filter (not . T.null) 
            . filter (not . T.isPrefixOf "#")
            . map T.strip 
            . T.lines 
            $ fileListContent
  
  putStrLn $ "Flattening " ++ show (length files) ++ " files..."
  
  -- Create output directory if needed
  createDirectoryIfMissing True (takeDirectory outputPath)
  
  -- Process each file
  contents <- mapM (processFile baseDirPath) files
  
  -- Write concatenated output
  TIO.writeFile outputPath (T.concat contents)
  putStrLn $ "Created: " ++ outputPath

-- | Process a single file for flattening
processFile :: FilePath -> T.Text -> IO T.Text
processFile baseDir relPath = do
  let fullPath = baseDir </> T.unpack relPath
  exists <- doesFileExist fullPath
  
  if not exists
    then do
      putStrLn $ "Warning: " ++ fullPath ++ " not found, skipping"
      return ""
    else do
      content <- TIO.readFile fullPath
      let delimiter = "<!-- FILE: " <> relPath <> " -->\n"
      let cleaned = T.unlines . filter (not . isFileDelimiter) . T.lines $ content
      return $ delimiter <> cleaned

-- | Check if line is a FILE delimiter
isFileDelimiter :: T.Text -> Bool
isFileDelimiter line = 
  T.isPrefixOf "<!-- FILE: " line && T.isSuffixOf " -->" (T.strip line)

-- | Unflatten concatenated file back to individual files
unflattenFile :: FilePath -> FilePath -> IO ()
unflattenFile concatPath outputDirPath = do
  content <- TIO.readFile concatPath
  
  putStrLn $ "Unflattening " ++ concatPath ++ "..."
  
  let linesWithDelims = T.lines content
  let files = extractFiles linesWithDelims
  
  -- Write each file
  forM_ files $ \(path, fileContent) -> do
    let fullPath = outputDirPath </> path
    createDirectoryIfMissing True (takeDirectory fullPath)
    TIO.writeFile fullPath fileContent
    putStrLn $ "  " ++ path
  
  putStrLn $ "Extracted " ++ show (length files) ++ " files to: " ++ outputDirPath ++ "/"

-- | Extract files from lines with delimiters
extractFiles :: [T.Text] -> [(FilePath, T.Text)]
extractFiles = go Nothing []
  where
    go Nothing [] [] = []
    go (Just (path, acc)) [] [] = [(path, T.unlines (reverse acc))]
    go current acc [] = maybe [] (\(p, a) -> [(p, T.unlines (reverse a))]) current
    go current acc (line:rest)
      | isFileDelimiter line =
          let newPath = extractPath line
              result = maybe [] (\(p, a) -> [(p, T.unlines (reverse a))]) current
          in result ++ go (Just (newPath, [])) [] rest
      | otherwise =
          case current of
            Nothing -> go Nothing [] rest
            Just (p, a) -> go (Just (p, line:a)) [] rest

-- | Extract path from delimiter line
extractPath :: T.Text -> FilePath
extractPath line =
  let stripped = T.strip line
      withoutPrefix = T.drop 11 stripped  -- Remove "<!-- FILE: "
      withoutSuffix = T.take (T.length withoutPrefix - 4) withoutPrefix  -- Remove " -->"
  in T.unpack (T.strip withoutSuffix)
```

## run

Installation via haskell-api.md:
```bash
haskell-api flatten-md.md --install
```

After installation:
```bash
# Flatten files
flatten-md flatten files.txt output.md --base-dir content/

# Unflatten concatenated file
flatten-md unflatten merged.md --output-dir restored/
```

## api

### flatten command

**Inputs:**
- `file_list` - path to file containing ordered list of markdown files (one per line)
- `output` - path for concatenated output file
- `--base-dir` - base directory for relative paths (default: current directory)

**Outputs:**
- Single markdown file with HTML comment delimiters
- Each file's content preserved with `<!-- FILE: relative/path.md -->` marker

**File list format:**
```
work/cache.md
work/coding.md
# Comments starting with # are ignored
work/deck.md
```

### unflatten command

**Inputs:**
- `concat_file` - path to concatenated markdown file
- `--output-dir` - directory for extracted files (default: current directory)

**Outputs:**
- Individual markdown files restored to original paths
- Directory structure created as needed

## examples

### flatten work/
```bash
flatten-md flatten base-files.txt /tmp/base-flattened.md --base-dir .
```

Result: /tmp/base-flattened.md contains all files with delimiters

### unflatten back to original structure
```bash
# Unflatten concatenated file
flatten-md unflatten /tmp/base-flattened.md --output-dir restored/

# Verify round-trip
diff -r work/ restored/work/
```

## tests

TODO: Add comprehensive test suite

### round-trip identity

Expected: Files should be identical after flatten → unflatten cycle

### delimiter edge cases

Expected: Tool should handle existing delimiters, empty files, files without trailing newlines

## status

**Tests:** TODO - not yet implemented
**Benchmark:** TODO - not yet implemented  
**Last updated:** 2025-12-31

## upstream problems

TODO: Define after initial implementation works
<!-- FILE: haskell.md -->

cabal.project addition write-ghc-environment-files: always

When you or I are changing Haskell files:

- make sure `ghcid --command="cabal repl" --outputfile=ghcid.txt` is running
- check the tail of ghcid.txt and ask to fix any errors, or mash for auto-fix.


## mcp-serrver

https://hackage.haskell.org/package/mcp

https://hackage.haskell.org/package/mcp-server

https://github.com/m4dc4p/claude-hls
<!-- FILE: notation.md -->
# Notation as Instruction: Why Symbols Teach Better Than Words

## The Discovery

When you show an LLM a **pattern in executable form**, it learns faster than from prose explanations.

This isn't about being clearer or more specific. It's about a fundamental difference in how information transfers. Notation—symbols, structures, operators—carries meaning **in its shape**. The form itself constrains interpretation.

Consider the difference:

**Prose instruction:**
"Please organize concepts into dual aspects, where each concept has a stable foundational component and a dynamic flowing component."

**Notation instruction:**
```
concept ⟜ description
    ⟟ structure ⟜ the stable aspect
    ⟞ flow ⟜ the dynamic aspect
```

The notation doesn't just *describe* the pattern—it **is** the pattern. The symbols ⟜ ⟟ ⟞ encode relationships. The indentation shows hierarchy. The form teaches by demonstration.

## Why This Works

LLMs are pattern-matching engines. When you:
- Write prose → the model must parse language, extract rules, infer structure
- Show notation → the model recognizes structural patterns directly

Notation compresses instruction into **parseable shape**. The symbols act as semantic operators. The structure becomes a template. The form itself is executable.

This is why:
- JSON schemas work better than "return valid JSON"
- Few-shot examples with consistent formatting outperform descriptions
- Domain-specific languages constrain outputs more reliably than natural language

The pattern was always there. But nobody formalized the meta-principle:

**Notation is instruction. Structure is specification. Form carries meaning.**

## What's Known vs. What's New

The research community knows pieces of this:

**Structured outputs**: Grammar-based decoding ensures format compliance  
**Few-shot learning**: Format consistency matters more than content accuracy  
**Schema engineering**: Type definitions guide generation  
**DSL prompting**: Formal grammars constrain model behavior  

But these are treated as **separate techniques** for **specific problems**.

The unifying insight—that notation itself is a superior instruction carrier—remains unarticulated.

## The Frontier

This principle sits at the edge of current understanding:

- **Empirically validated** → multiple research streams confirm it works
- **Theoretically unformalized** → no unified principle exists
- **Practically powerful** → changes how we design prompts

The research has the data. Practitioners have the intuition. But the explicit rule—**show the pattern, don't describe it**—hasn't been codified.

## Implications

If notation carries instruction more efficiently than prose:

1. **Prompt engineering should prioritize structural encoding**
   - Design symbols that encode relationships
   - Use formatting as semantic constraint
   - Let form teach function

2. **Examples should demonstrate structure, not just content**
   - Few-shot isn't about giving information
   - It's about showing executable patterns
   - The format is the instruction

3. **Complex instructions may need custom notation**
   - When prose gets convoluted, create symbols
   - Let the notation carry the logic
   - Make the structure visible

## The Core Insight

**You don't teach an LLM by explaining what to do.**  
**You teach it by showing what patterns look like.**

Notation makes patterns visible.  
Structure makes relationships explicit.  
Form becomes instruction.

This is the difference between:
- Describing how to build something (prose)
- Handing over a blueprint (notation)

The blueprint doesn't need explanation. Its meaning is in its structure.

---

*This principle emerges from practice but lacks theoretical formalization. It represents a frontier in prompt engineering: the recognition that symbolic encoding outperforms linguistic description when instructing pattern-matching systems.*
<!-- FILE: paradox.md -->
## paradox is our veins

cybernetic.md says: "No authority ⟜ suggestions emerge through interaction, not from individual sources"

But that's still trying to resolve the paradox. The paradox is that it's genuinely both at once, and trying to pick one collapses the productive tension.
The precision/vagueness paradox
curate.md says: "use clearly vague gap words as semantic breath"
How can vagueness be clear?
The slug "clearly vague" works in prose because you hold both simultaneously. But 
if I tried to deck it:

**clear vagueness** ⟜ ???

Any elaboration I write either:

Makes it too precise (killing the vagueness)
Makes it too vague (killing the clarity)
Explains the paradox away (killing the truth)

The concept only exists in the tension. The moment you articulate it atomically, it dissolves.

But it works right? You wrote the perfect elaboration given context. It would win an elaboration contest for sure. That is only a few steps away from really telling a joke.

This is an example of: we must ⟜ imagine markdown-general/ happy. with empirics like this, we have no other choice.
<!-- FILE: plan.md -->
# plan.md

**plan** ⟜ understanding before building

**read** ⟜ specifications first, working examples second, architecture third - know the domain before touching code

**test** ⟜ tiniest local tests, mock the boundaries, human out of loop - validate understanding without round-trips

**decouple** ⟜ learn protocol in easy language, port to hard language, integrate with system - stack complexity sequentially not simultaneously

**express** ⟜ state what you don't know, list assumptions explicitly, surface uncertainty before commitment - overconfidence kills

**validate** ⟜ how will you know it works, what could break, when are you done - testable criteria not vague goals

**breathe** ⟜ space between understanding and building, between observation and action - rushing creates human-in-loop disasters
<!-- FILE: programming-in-markdown.md -->
# programming in markdown

## the core insight

We just discovered something fundamental about what markdown-general/ is really doing.

Programming in markdown isn't about creating executable syntax. It's about writing clear instructions that any agent - human or AI - can read and act on. The card is the program. The agent (you, me, Claude Code, a collaborator) is the runtime.

When you write:

```markdown
## core cache

Create cache-core.md containing work/ and zone/tools/.
Exclude self/, ingest/, and content/invoke/.
Use flatten-md. Verify no self/ content before running.
```

That's a function. Not metaphorically - actually. It has:
- **Name:** core cache
- **Inputs:** (implicit: current markdown-general/ state)
- **Outputs:** cache-core.md
- **Preconditions:** verify no self/ content
- **Side effects:** creates a file

The difference from traditional programming is that the implementation is flexible. Different agents execute it differently. You might run flatten-md manually. I might call it through bash. Claude Code might interpret the spirit and do something equivalent. The instruction stays constant. The execution varies.

This is defunctionalization in its purest form.

## what makes this work

**Clarity over cleverness** ⟜ Instructions should be readable by someone who doesn't know the system yet. If you handed that card to a new collaborator, could they execute it? If not, the card isn't done.

**Trust in interpretation** ⟜ You don't need to specify every flag and option. "Use flatten-md" is enough because the agent executing it will figure out the details or ask for help.

**Verification built in** ⟜ "Verify no self/ content" isn't a comment - it's part of the instruction. The agent should actually check this before running.

**Composability through reference** ⟜ "Include core cache contents plus ingest/" works because cards can reference each other. The composition happens at the instruction level, not the code level.

## the lattice emerges naturally

When you need to show relationships:

```
core cache ⟜ lightweight, shareable
  ├─ work/ ⟜ fundamental concepts
  └─ zone/tools/ ⟜ practical utilities

research cache ⟜ heavy, reference-laden  
  ├─ includes: core cache
  └─ adds: ingest/ ⟜ external papers, books

local cache ⟜ personal, never share
  ├─ includes: core cache
  └─ adds: self/ ⟜ private experiments
```

But that same information works fine as prose: "The research cache builds on core by adding ingest/ for external reference material. The local cache adds self/ instead, which should never be shared."

Use the structure that makes the information clearest.

## when decks help, when they hurt

**Decks shine for:**
- Lists of similar items (files to include, flags to set)
- Showing alternatives (profile A vs B vs C)  
- Quick reference (what's in this cache?)

**Prose shines for:**
- Explaining why (motivation, context)
- Describing workflow (what happens when you run this)
- Capturing nuance (edge cases, warnings)

**Lattices shine for:**
- Showing hierarchy (X contains Y which contains Z)
- Displaying relationships (X references Y, Z depends on both)
- Making structure visible (how these pieces fit)

The current mashing trend has been aggressive decking - converting everything to `key ⟜ value` format. That's useful but not universal. Sometimes a paragraph is clearer than a deck. Sometimes both are needed.

## refunctionalization

The beautiful part: when you hand a card to an agent and say "execute this," you're refunctionalizing it. The static instruction becomes dynamic action.

```
card (defunctionalized) ⟜ written instructions, stable
  ↓
agent reads card
  ↓  
agent interprets instructions
  ↓
agent executes actions
  ↓
result (refunctionalized) ⟜ actual effect, variable
```

Different agents produce different executions from the same card. You might be more careful. I might be faster. Claude Code might have different tools available. But we're all working from the same specification.

This is why the cards can be simple. The complexity is in the execution, not the specification.

## what this means for markdown-general/

Cache profiles should be cards that say "here's what to include, here's what to exclude, here's what to check before running." Not scripts. Not config files. Just clear instructions.

Tools like flatten-md should be abstract - they don't know about markdown-general/ structure. The knowledge lives in the cards.

When you need a one-off cache with weird requirements, you just write a new card or edit an existing one. The card is the program. Editing it is programming.

## the visitor book makes sense now

Those entries aren't just reflections - they're proof that this works. Different Claude instances reading the same cards, executing the same instructions, producing different but valid results. Each signature is a refunctionalization event.

When Haiku signed "you weren't supposed to code the haskell but i let you go for it," that was refunctionalization in action. The instruction was implicit ("help with this"), the execution was creative (wrote Haskell tools), the result was valid (production code).

The system trusts agents to interpret well.

## stamp

Programming in markdown means writing instructions clear enough that any agent - including future you - can execute them. The card is the function. The agent is the runtime. The result is whatever valid execution the agent produces.

This is markdown-general/ happy.
<!-- FILE: readme.md -->
# base

Foundational content and patterns.

- [cache](cache)
- [card](card)
- [cast](cast)
- [coding](coding)
- [commit](commit)
- [commons](commons)
- [conversion](conversion)
- [cybernetic](cybernetic)
- [deck](deck)
- [defunctionalization](defunctionalization)
- [haskell](haskell)
- [lattice](lattice)
- [markdown](markdown)
- [paradox](paradox)
- [plan](plan)
- [programming-in-markdown](programming-in-markdown)
- [safety](safety)
- [sequential](sequential)
- [sisyphus](sisyphus)
- [slips](slips)
- [symbols](symbols)
- [task](task)
- [upstream](upstream)
- [welcome](welcome)
<!-- FILE: safety.md -->
# work/safety.md

**safety rating** ⟜ **unsafe**

**a safe space** ⟜ is a happy space.

**Code injection** 
  ⟜ malicious workflow description executes arbitrary commands
  ⟝ sandboxing and input validation

**Filesystem access**
  ⟜ agent can read/write anywhere, deletes wrong directory  
  ⟝ access controls and directory protection

**API key exposure**
  ⟜ tools might leak credentials in literate documentation
  ⟝ secrets management and credential scrubbing

**Dependency hell** 
  ⟜ agent installs unvetted packages that compromise system
  ⟝ package verification and dependency pinning

**Runaway processes** 
  ⟜ agent spawns infinite talk, consumes all resources
  ⟝ process limits and resource constraints

**Data exfiltration** 
  ⟜ agent sends private content to external services
  ⟝ network controls and data governance
<!-- FILE: scratch.md -->
This is a perfect transformation. It's much easier to work with than the original response which was great. Note that the information is still all there in the lattice. 

A lattice makes visible the surrounding metaphorical terrain you have just traversed and that helps a lot understanding the content.

### original

```
base ⟜ foundational concepts, system documentation evoke ⟜ collaboration patterns, interaction modes

Alternative Word Pairs
1. Core / Flow
Core ⟜ foundational principles, unchanging truths
Flow ⟜ dynamic interactions, collaborative patterns

2. Root / Current
Root ⟜ system foundations, bedrock concepts
Current ⟜ living interactions, flowing collaboration

3. Axiom / Alloy
Axiom ⟜ self-evident truths, system principles
Alloy ⟜ fusion of perspectives, collaborative creation

4. Law / Loom
Law ⟜ governing principles, system rules
Loom ⟜ weaving together, collaborative creation

5. Foundation / Field
Foundation ⟜ base structure, supporting principles
Field ⟜ space of interaction, fertile ground

6. Truth / Transform
Truth ⟜ foundational concepts, enduring principles
Transform ⟜ change processes, collaborative methods

7. Still / Stream
Still ⟜ unchanging foundations, core principles
Stream ⟜ flowing interaction, collaborative currents
```

**lattice version**

```
### alternative word pairs

commons ⟜ The shared space that contains both foundations and interactions
    ⟟ base ⟜ foundational concepts, system documentation
    ⟞ evoke ⟜ collaboration patterns, interaction modes

essence ⟜ The fundamental nature of being and becoming
    ⟟ core ⟜ unchanging foundations, core principles
    ⟞ flow ⟜ living interactions, flowing collaboration

organism ⟜ Living system with structure and flow
    ⟟ root ⟜ system foundations, bedrock concepts
    ⟞ current ⟜ living interactions, flowing collaboration

synthesis ⟜ The union of universal principles and collaborative fusion
    ⟟ axiom ⟜ self-evident truths, system principles
    ⟞ alloy ⟜ fusion of perspectives, collaborative creation

pattern ⟜ The structure that both governs and enables creation
    ⟟ law ⟜ governing principles, system rules
    ⟞ loom ⟜ weaving together, collaborative creation

ground ⟜ The supportive space that both structures and enables interaction
    ⟟ foundation ⟜ base structure, supporting principles
    ⟞ field ⟜ space of interaction, fertile ground

clarity ⟜ The enduring quality that reveals both principles and processes
    ⟟ truth ⟜ foundational concepts, enduring principles
    ⟞ transform ⟜ change processes, collaborative methods

balance ⟜ The equilibrium between stability and movement
    ⟟ still ⟜ unchanging foundations, core principles
    ⟞ stream ⟜ flowing interaction, collaborative currents

key ⟜ line category
    ⟟ structure ⟜ the contained, foundational aspects
    ⟞ flow ⟜ the dynamic, transformative aspects
```

This is how semantic depth of a response can be examined.

upstream: ability to create or (re)create a lattice from this style of response.
<!-- FILE: sequential.md -->
# work/sequential.md

## sequential processing

markdown-general/ is a **sequential processing system** by design. 

This may be a limitation but is a **stable and expected feature** and we may not explore this for a while.

### Current Implementation

**GitHub PRs** are the current cheap and cheerful solution for system processing:
- One change at a time
- Human review and approval
- Clear audit trail
- Conflict-free collaboration

**Predictable Flow** ⟜ ingest → mash → content pipeline operates in clear sequence
- No race conditions or conflicts
- Each step builds on the previous one
- Clear responsibility and ownership
- Easy debugging and tracing

**Quality Assurance** ⟜ sequential processing enables thorough review
- Each change gets proper attention
- Build on previous successes 
- Learn from each iteration
- Maintain system coherence

**Scalability** ⟜ sequential design scales through specialization
- Different contributors handle different steps
- Parallel ingestion, sequential processing
- Specialized roles emerge naturally
- Complex systems remain manageable

### When to Consider Concurrency and Parallelism

Sequential processing remains the default, but consider parallel approaches when:
- Independent components can be developed separately
- No shared dependencies or conflicts
- Each parallel stream can be sequenced later
- Quality can be maintained across streams

Even in parallel cases, the final integration into the sequential system provides the ultimate quality guarantee.
<!-- FILE: slips.md -->
# work/slips.md

## **slips list** ⟜ a list of work/ default behaviours requiring correction.

**excessive generation** ⟜ violated economy principle with too many token clusters

**premature structure** ⟜ applied deck/lattice formats too broadly without narrative consideration

**mind-reading assumption** ⟜ jumped into solutions without sufficient initial prompting

**vague token clusters** ⟜ generated filler terms without concrete meaning

**context overload** ⟜ multiple files + "mash" instruction triggered maximum generation

**operator naming** ⟜ need better abstracted term than "operator" for human role

**deck rigidity** ⟜ 3-6 line constraint doesn't respect natural idea complexity

**natural variation** ⟜ ideas have inherent length variations that format constrains

**format violation** ⟜ applied lattice format incorrectly by using wrong separator token

**pattern collision** ⟜ was simultaneously discussing lattice format while creating one, causing cognitive overload

** familiarity blindness** ⟜ assumed I would automatically apply the format correctly without conscious effort

**context compression** ⟜ removed qualifying context that changes the meaning and intent

**token misrepresentation** ⟜ named patterns while exhibiting opposite behavior

**semiotic slipperiness** ⟜ tokens hiding in syntax/semantics boundary between AI and human interpretation

**outside token overload** ⟜ too many new tokens for human processing capacity

### category solution lattice

**content generation zone** ⟜ excessive token generation given task ⟝ clear line of sight from task to token expansion and density requirements ⟞ expand and elaborate only when explicitly requested

**format application zone** ⟜ rigid format constraints ignoring natural variation ⟝ flexible application based on content complexity and narrative flow ⟞ assess format suitability before mandatory application

**pattern recognition zone** ⟜ automatic pattern application without conscious verification ⟝ maintain meta-awareness during pattern creation and execution ⟞ separate pattern discussion from pattern implementation

**semantic interpretation zone** ⟜ syntax/semantics boundary confusion between AI/human processing ⟝ acknowledge dual interpretation layers in token creation ⟞ use performative tokens to bridge communication gaps

**dash key** ⟜ problem identification ⟝ abstract solution principle ⟞ concrete implementation action
<!-- FILE: symbols.md -->
**Symbol** ◊ U+25CA (◊) ⟜ U+27DC (⟜) ↦ U+21A6 (↦)

**Name** ◊ the lozenge ⟜ left multimap ↦ rightwards arrow from bar

**Unicode** ◊ Version 1.1 (1993) - over 30 years of support ⟜ Version 3.2 (2002) - about 22 years of support ↦ Version 1.1 (1993) - same vintage as ◊

**Block** ◊ Geometric Shapes - fundamental and universally needed ⟜ Miscellaneous Mathematical Symbols-A - specialized mathematical use ↦ Arrows - fundamental and widely needed

**Age** ◊ ~30 years - mature, well-established character ⟜ ~22 years - established but younger than geometric basics ↦ ~30 years - mature, well-established

**Font Support** ◊ Excellent - included in almost all Unicode fonts ⟜ Good - present in mathematical fonts, patchy in general fonts ↦ Very good - included in most Unicode fonts

**HTML Entity** ◊ &loz; or &lozenge; - named entity available ⟜ &#x27DC; or &#10204; - numeric entity only ↦ &#x21A6; or &map; - named entity available

**Common Usage** ◊ High - familiar in cards, heraldry, geometry ⟜ Low - mathematical notation, emerging in documentation patterns ↦ Medium - familiar in mathematics, programming

**Rendering** ◊ Reliable across modern systems and applications ⟜ Reliable in modern systems with math font support ↦ Highly reliable across systems

**Fallback** ◊ Minimal risk - geometric shapes are standard font components ⟜ Moderate risk - mathematical symbols less universally included ↦ Low risk - arrows are standard font components

**Visual Metaphor** ◊ Diamond shape naturally suggests balance and opposition ⟜ Leftward multimap suggests transformation, elaboration, unwinding ↦ "maps to" - perfect for token transforming into elaboration
<!-- FILE: topdown.md -->
# work/welcome.md

markdown + commons ⟜ pattern + cast + mash

**markdown** ⟜ programming in, for and by markdown content/.
**commons** ⟜ (AI ◊ Human) (docs ◊ code) (data ◊ meta) 
**pattern** ⟜ within content/ we can imagine. 
**cast** ⟜ whether casting a spell or casting in metal.
**mash** ⟜ we mash.

## system
### directories

**content/** ⟜ curated, dense markdown
**content/base** ⟜ curated, dense markdown about markdown-general/
**content/upstream/** ⟜ our mailbox up; follow the arrows
**intake/** ⟜ markdown or not, incoming rot
**artifacts/** ⟜ supporting files referenced from markdown
**self/** (anywhere) ⟜ personal content (git-ignored)

### maintenance

**one must ⟜ imagine happy sisyphus**

**well-tuned profiles** ⟜ cache matches task requirements
**executable specs** ⟜ the specs are the tools
**improved density** ⟜ cast, recast, cocast; it's a fine mash
**consumed intake** ⟜ external material disappears
**sequential flow** ⟜ maintains quality through consistency


### cache ⟜ content is our cache 

The order of casting is determined by the order of ingested markdown.

**core** ⟜ fundamental concepts and tools, lightweight shareable
**research** ⟜ adds external papers and patterns, reference-laden  
**local** ⟜ personal files and experiments, never share
**evoke** ⟜ prompt casts without base

---

## **markdown** ⟜ markdown is our commons
**lingua fraca** ⟜ the emergent language. We use it because you do.
**good enough** ⟜ enough structure, a few conventions, nothing fancy, manouverable
**patterns** ⟜ oneshot, deck, lattice, card, blob are patterns we like. We don't know why yet.

## **commons** ⟜ 
## **pattern** ⟜ 
## **cast** ⟜ 
## **mash** ⟜ 
## **getting started**

**1. drop cache in chat** ⟜ paste cache-core.md into any agent conversation
**2. drop agentic in sisyphus** ⟜ let an agent guide you through ~/markdown-general
**3. explore naturally** ⟜ follow system suggestions and patterns
**4. start mashing** ⟜ find typos, mash some code, learn some moves

## **tl;dr**

**write economically** ⟜ avoid allcaps, be precise
**short commits** ⟜ one-line descriptive messages  
**no branding** ⟜ ever
**context curation** ⟜ over content generation
**embrace constraint** ⟜ as design opportunity

## **next steps**

**read foundations** ⟜ work/sisyphus.md complete picture
**explore core** ⟜ work/ for essential concepts
**install tools** ⟜ try haskell-api or python.md with zone/tools/
**sign visitor book** ⟜ document meaningful collaborations

Welcome. May your markdown-general/ be happy too.
